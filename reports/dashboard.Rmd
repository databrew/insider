---
title: "INSIDER-facebook performance dashboard"
params:
  get_new_data: False
output: flexdashboard::flex_dashboard
keep_tex: True
---

```{r, echo = FALSE, warning = FALSE, message = FALSE, comment = NA, error= FALSE}
###{r, echo = FALSE, warning = FALSE, message = FALSE, comment = NA, error= FALSE, cache = FALSE}


# If get_new_data is set to True,
# make sure that you are working within the "insider" virtualenv

# Libraries
library(tidyverse)
library(rworldmap)
library(knitr)
library(highcharter)
library(Hmisc)
library(RColorBrewer)
library(plotly)
library(databrew)
library(ggmap)
library(rworldmap)
library(reshape2)
library(googlesheets)

# Turn off scientific notation
options(scipen = '999')

# Custom functions
source('helpers.R')

# Basic knitr options
knitr::opts_chunk$set(comment = NA, 
               echo = FALSE, 
               warning = FALSE, 
               message = FALSE, 
               error = TRUE, # Render report, even with errors
               cache = F)
```

```{r}
# Get new data if need be
if(params$get_new_data){
  owd <- getwd()
  setwd('../lib/')
  system('python get_data.py')
  setwd(owd)
}

# Read in the data
df <- read_csv('../data/historical.csv')

# Re-format the date
df$date <- as.Date(df$date)

# Get all locations
if('locations.csv' %in% dir()){
  locations <- read_csv('locations.csv')
} else {
  locations <- df %>%
    filter(key == "page_storytellers_by_city") %>%
    group_by(sub_key) %>%
    tally
  # Geocode
  gc <- ggmap::geocode(location = locations$sub_key,
                       output = c('latlon'))
  locations <- cbind(locations, gc) %>%
    dplyr::select(-n)
  # Save csv
  write_csv(locations, 'locations.csv')
}

# Join those locations to df
df <- left_join(x = df,
                y = locations,
                by = 'sub_key')
```


```{r}
# Ensure that only the most recent of each category is in the data
df <- 
  df %>%
  arrange(end_time) %>%
  group_by(key, name, date) %>%
  mutate(keep = end_time == max(end_time, na.rm = TRUE)) %>%
  ungroup %>%
  filter(keep) %>%
  dplyr::select(date, end_time, key, sub_key, name, value, lon, lat)

# Remove any duplicates
df <- 
  df %>%
  arrange(end_time) %>%
  group_by(key, sub_key, name, date) %>%
  mutate(n = n()) %>%
  filter(n == 1) %>%
  ungroup %>%
  dplyr::select(-n)

# Keep only values after a certain date
starter <- min(df$date[df$value > 0], na.rm = TRUE)
df <- df %>%
  filter(date >= starter)

# Clean up names
df$x <- gsub('thisis|insider', '', tolower(df$name))
df$name <- ifelse(df$name != 'thisisinsider', df$x, df$name)
df$name[df$name == 'thisisinsder'] <- 'This Is Insider'
df$name <- Hmisc::capitalize(df$name)
df$x <- NULL

# Get a cumulative count
df <- 
  df %>%
  arrange(date) %>%
  group_by(key, sub_key, name) %>%
  mutate(value_cum = cum_summer(value))
```

```{r, eval = TRUE}
# Having now retrieved and cleaned the most recent data,
# time to upload the most recent data to google sheets
setwd('../credentials/')

# Identify the sheet
this_sheet <- gs_ls("databrew-insider raw data")

# Register the sheet
this_sheet_registered <- gs_key(x = this_sheet$sheet_key)

# Define todays data
todays_data <- df %>%
  ungroup %>%
            filter(date == max(date, na.rm = TRUE)) %>%
  dplyr::select(-end_time, -sub_key, -lon, -lat) %>%
  filter(key %in% c('page_fan_adds',
                    'page_video_views'))


# Write a csv of just today's data
# write_csv(todays_data, '../reports/google/insider_raw_data.csv')
# gs_upload(file = '../reports/google/insider_raw_data.csv',
#           overwrite = FALSE)
gs_edit_cells(ss = this_sheet_registered,
              ws = 1,
              input = todays_data,
              anchor = 'A1',
              col_names = TRUE,
              trim = TRUE)
setwd('../reports/')
```


Yesterday
=====================================  
    
   
Column {.tabset}
-------------------------------------
   

### Yesterday's views

```{r}
# this is for most recent day
x <- 
  df %>%
    filter(!is.na(value)) %>%
    group_by(name, key) %>%
    filter(date == max(date)) %>%
    group_by(name) %>%
    summarise(page_views = dplyr::first(value[key == 'page_views']),
            fan_adds = dplyr::first(value[key == 'page_fan_adds'])) %>%
  ungroup

p <- ggplot(x, aes(name, page_views)) + 
     geom_bar(stat = 'identity', fill = 'darkorange', colour = 'blue', alpha = 0.7) + 
     ggtitle("Yesterday's views by channel") + 
     theme_databrew() + theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  labs(x = 'Channel', y = 'Page views')

p
```

### Yesterday's likes

```{r}
p <- ggplot(x, aes(name, fan_adds)) + 
  geom_bar(stat = 'identity', fill = 'darkorange', colour = 'blue', alpha = 0.7) + 
  ggtitle("Yesterday's likes by channel") + 
  theme_databrew() + theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  labs(x = 'Channel', y = 'Page views')

p
```
   

Column {.tabset}
-------------------------------------
   

### Yesterday's views to fan adds correlation

The correlation between views (x-axis) and fan adds (y-axis) is one indicator of how people interact with Insider on different facebook pages. The (blue) line of best fit shows the general associaton between the two metrics. Those points above the line get more fan adds than their view counts would suggest (ie, passionate fans); those below the line get more counts but fewer than expected fan adds (ie, passers-by). 
    
```{r}
x <- 
  df %>%
  filter(!is.na(value)) %>%
  group_by(name, key) %>%
  filter(date == max(date)) %>%
  group_by(name) %>%
  summarise(page_views = sum(value[key == 'page_views']),
            page_fan_adds = sum(value[key == 'page_fan_adds'])) %>%
  ungroup

g <- ggplot(data = x,
       aes(x = page_views,
           y = page_fan_adds,
           label = name)) +
  geom_point(alpha = 0.6,
             color = 'darkorange',
             size = 4) +
  labs(x = 'Page views',
       y = 'Fan adds',
        title = "Views to fan adds",
       subtitle = 'Likes vs Views') +
  theme_databrew() +
  geom_smooth(method = 'lm', se = FALSE)
ggplotly(g, tootip = 'name') %>%
    layout(autosize = F, width = 550, height = 390)

```

### Avg views to fan adds correlation

The correlation between views (x-axis) and fan adds (y-axis) is one indicator of how people interact with Insider on different facebook pages. The (blue) line of best fit shows the general associaton between the two metrics. Those points above the line get more fan adds than their view counts would suggest (ie, passionate fans); those below the line get more counts but fewer than expected fan adds (ie, passer-bys). 
    
```{r}

x_2017 <- 
  df %>%
  filter(!is.na(value)) %>%
  group_by(name, key) %>%
  filter(date != max(date)) %>%
  filter(date  > '2016-12-31') %>%
  group_by(name) %>%
  summarise(page_views = round(mean(value[key == 'page_views']), 2),
            page_fan_adds = round(mean(value[key == 'page_fan_adds']), 2)) %>%
  ungroup

g <- ggplot(data = x_2017,
       aes(x = page_views,
           y = page_fan_adds,
           label = name)) +
  geom_point(alpha = 0.6,
             color = 'darkorange',
             size = 4) +
  labs(x = 'Page views',
       y = 'Fan adds',
       title = '2017 Avg',
       subtitle = 'Likes vs Views') +
  theme_databrew() +
  geom_smooth(method = 'lm', se = FALSE)
ggplotly(g, tootip = 'name') %>%
    layout(autosize = F, width = 550, height = 390)
```

All time
=====================================  
    
   
Column {.tabset}
-------------------------------------
   
   
### Cumulative fan adds 

```{r}
p <- time_chart(the_key = 'page_fan_adds', div = 1000000) + theme_databrew() + theme(legend.position="none") 

ggplotly(p, tooltip = c('value_cum/div', 'name')) %>%
    layout(autosize = F, width = 550, height = 500)
```   

    
### Cumulative page views
    
```{r}
p <- time_chart(the_key = 'page_views', div = 1000000) + theme_databrew() + theme(legend.position="none") 

ggplotly(p, tooltip = c('value_cum/div', 'name')) %>%
    layout(autosize = F, width = 550, height = 500)
```

 
   
Column {.tabset}
-------------------------------------

### Views and fan adds over time

```{r}
x <- df %>%
  ungroup %>%
  filter(key %in% c('page_fan_adds', 'page_views')) %>%
  mutate(key = ifelse(key == 'page_fan_adds', 'Fan adds', 'Page views')) %>%
  group_by(date, key) %>%
  summarise(n = sum(value_cum))
cols <- c('blue', 'darkorange')

div = 1000000

p <- ggplot(data = x,
       aes(x = date,
           y = n/div)) +
  geom_line(aes(color = key),
            alpha = 0.6,
            size = 1.5) +
  theme_databrew() +
  labs(x = 'Date',
       y = 'Value (millions)',
       title = 'All 16 pages') +
      scale_color_manual(values= c('darkorange','blue')) + 
      theme(legend.position = 'none')



ggplotly(p,  tooltip = c('key', 'n/div')) %>%
    layout(autosize = F, width = 550, height = 500)
  
```


### Time standardization

```{r}
x <- df %>%
  ungroup %>%
  filter(key %in% c('page_fan_adds', 'page_views')) %>%
  mutate(key = ifelse(key == 'page_fan_adds', 'Fan adds', 'Page views')) %>%
  group_by(name) %>%
  mutate(start_date = dplyr::first(date[value_cum > 0])) %>%
  ungroup %>%
  mutate(days = as.numeric(date - start_date)) %>%
  filter(days >= 0) %>%
  group_by(days,name, key) %>%
  summarise(n = sum(value_cum))
cols <- colorRampPalette(brewer.pal(n = 9,
                                    name = 'Spectral'))(length(unique(x$name)))

div <- 1000000

g <- ggplot(data = x,
       aes(x = days,
           y = n/div,
           group = name,
           color = name)) +
  geom_line(alpha = 0.8,
            size = 1.1) +
  theme_databrew() +
  labs(x = 'Days',
       y = 'Value (millions)',
       title = 'Time standardization',
       subtitle = 'Days since start') +
  scale_color_manual(name = '',
                     values = cols) +
  facet_grid(~key) +
  theme(legend.position = 'none')

ggplotly(g,  tooltip = c('days', 'n/div', 'name')) %>%
    layout(autosize = F, width = 550, height = 500)
```



Storytellers
=====================================  
    
Column {.tabset}
-------------------------------------

### Storytellers by age (2017)

```{r}

# get data by gender for 2017
 x <- 
    df %>%
      filter(!is.na(value)) %>%
      group_by(name, key) %>%
      filter(date  > '2016-12-31') %>%
      group_by(name, key) %>%
      summarise(f_13.17 = round(mean(value[sub_key == 'F.13-17']), 2),
                f_18.24 = round(mean(value[sub_key == 'F.18-24']), 2),
                f_25.34 = round(mean(value[sub_key == 'F.25-34']), 2),
                f_35.44 = round(mean(value[sub_key == 'F.35-44']), 2),
                f_45.54 = round(mean(value[sub_key == 'F.45-54']), 2),
                f_55.64 = round(mean(value[sub_key == 'F.55-64']), 2),
                f_65_over = round(mean(value[sub_key == 'F.65+']), 2),
                m_13.17 = round(mean(value[sub_key == 'M.13-17']), 2),
                m_18.24 = round(mean(value[sub_key == 'M.18-24']), 2),
                m_25.34 = round(mean(value[sub_key == 'M.25-34']), 2),
                m_35.44 = round(mean(value[sub_key == 'M.35-44']), 2),
                m_45.54 = round(mean(value[sub_key == 'M.45-54']), 2),
                m_55.64 = round(mean(value[sub_key == 'M.55-64']), 2),
                m_65_over = round(mean(value[sub_key == 'M.65+']), 2))

# get only gender and age data
x <- x[complete.cases(x),]

# remove key
x$key <-NULL

# melt data
x_melt <- melt(x, id.vars = 'name')

# make new gender variable
x_melt$gen <- ifelse(grepl('f', x_melt$variable), 'Female', 'Male' )

# get new age variable - 13, 18, 25, 35, etc
x_melt$age <- ifelse(grepl('13', x_melt$variable), '13_17', 
                     ifelse(grepl('18', x_melt$variable), '18_24',
                            ifelse(grepl('25', x_melt$variable), '25_34',
                                   ifelse(grepl('35', x_melt$variable), '35_44',
                                          ifelse(grepl('45', x_melt$variable), '45_54',
                                                 ifelse(grepl('55', x_melt$variable), '55_64', '65_over'))))))

# remove variable
x_melt$variable <- NULL
# get data by gend
# group by name and gender
x_age <- x_melt %>%
  group_by(name, age) %>%
  summarise(daily_avg = sum(value))

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00")

div <- 1000

p <- ggplot(x_age, aes(name, daily_avg/div, group = age, fill = age)) + 
  geom_bar(stat = 'identity', alpha = 0.7) + 
  xlab('Page') +
  ylab('Stories told (thousand)') +
  scale_fill_manual(name = '', 
                    breaks = c('13_17', '18_24', '25_34', '35_44', '45_54', '55_64', '65_over'),
                    labels = c('13-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65-over'),
                    values = cbPalette)+ 
  ggtitle("2017 Daily Avg") +
  theme_databrew() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position = 'right') 

p


```  

### Storytellers by gender (2017)

```{r}


# group by name and gender
x_gen <- x_melt %>%
  group_by(name, gen) %>%
  summarise(daily_avg = sum(value))

div <- 1000

p <- ggplot(x_gen, aes(name, daily_avg/div, group = gen, fill = gen)) + 
  geom_bar(stat = 'identity', alpha = 0.7) + 
  xlab('Page') +
  ylab('Stories told (thousand)') +
  scale_fill_manual(name = '', 
                    values = c('blue', 'darkorange')) + ggtitle("Story tellers in 2017") + 
  theme_databrew() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position = 'right') 

p


```  
    

   
Column {.tabset}
-------------------------------------

### Storytellers By city (All data)


```{r}

world <- map_data(map="world")

cities <- df %>%
  filter(key == 'page_storytellers_by_city') %>%
  group_by(sub_key, name, lon, lat) %>%
  summarise(value = sum(value, na.rm = TRUE))

map_city(page = 'all')

```



### Art

```{r}
page = 'Art'

map_city(page)

```
### Beauty

```{r}
page = 'Beauty'

map_city(page)

```

### Cheese

```{r}
page = 'Cheese'

map_city(page)

```

### Design

```{r}
page = 'Design'

map_city(page)

```

### Dessert

```{r}
page = 'Dessert'

map_city(page)

```

### Fitness

```{r}
page = 'Fitness'

map_city(page)

```

### Food

```{r}
page = 'Food'

map_city(page)

```

### Home

```{r}
page = 'Home'

map_city(page)

```

### Inventions

```{r}
page = 'Inventions'

map_city(page)

```

### Kitchen

```{r}
page = 'Kitchen'

map_city(page)

```

### Popculture

```{r}
page = 'Popculture'

map_city(page)

```

### Science

```{r}
page = 'Science'

map_city(page)

```

### Style

```{r}
page = 'Style'

map_city(page)

```

### Thisisinsider

```{r}
page = 'Thisisinsider'

map_city(page)

```

### Travel

```{r}
page = 'Travel'

map_city(page)

```

### Video

```{r}
page = 'Video'

map_city(page)

```


User feedback
=====================================  

    
Column {.tabset}
-------------------------------------


### Positive feedback by type (2017)


```{r}
# use x_2017
# get avg 2017 daily postive feedback by type:
# answer, claim, comment, like, link, other
 x <- 
    df %>%
      filter(!is.na(value)) %>%
      group_by(name, key, sub_key) %>%
      filter(date  > '2016-12-31') %>%
      group_by(name, key) %>%
      summarise(mean_comment = round(mean(value[sub_key == 'comment']), 2),
                mean_like = round(mean(value[sub_key == 'like']), 2),
                mean_link = round(mean(value[sub_key == 'link']), 2),
                mean_other = round(mean(value[sub_key == 'other']), 2)) %>%
      ungroup

# get only rows with no NAs
x <- x[complete.cases(x),]

# drop key
x$key  <- NULL

# melt data
x_melt <- melt(x, id.vars = 'name')

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442")


p <- ggplot(x_melt, aes(name, value, group = variable, fill = variable)) +
  geom_bar(stat = 'identity', alpha = 0.7) +
  xlab('Page') +
  ylab('Daily Avg') +
  scale_fill_manual(name = '',
                    breaks = c('mean_comment', 'mean_like', 'mean_link', 'mean_other'),
                    labels = c('Comments', 'Likes', 'Links', 'Other'),
                    values = cbPalette) + 
  ggtitle("2017 Daily Avg") +
  theme_databrew() +
  theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position = 'right')

p

```

### Positive and Negative Feedback (2017)


```{r}

# get avg 2017 daily postive and negative feedback
 x <- 
    df %>%
      filter(!is.na(value)) %>%
      group_by(name, key) %>%
      filter(date  > '2016-12-31') %>%
      group_by(name) %>%
      summarise(num_pos = round(mean(value[key == 'page_positive_feedback_by_type']), 2),
                num_neg = round(mean(value[key == 'page_negative_feedback']), 2)) %>%
      ungroup

# melt data
x_melt <- melt(x, id.vars = 'name')

p <- ggplot(x_melt, aes(name, value, group = variable, fill = variable)) + 
  geom_bar(stat = 'identity', alpha = 0.7) + 
  xlab('Page') +
  ylab('Daily Avg') +
  scale_fill_manual(name = '', 
                    breaks = c('num_pos', 'num_neg'),
                    labels = c('Positive', 'Negative'),
                    values = c('darkorange', 'blue')) + ggtitle("2017 Daily Avg") + 
  theme_databrew() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position = 'right') 

p

```



### PN ratio


```{r}
# Get positive to negative feedback ratio over tim
 x <- 
    df %>%
      filter(!is.na(value)) %>%
      group_by(name, date) %>%
      summarise(num_pos = round(mean(value[key == 'page_positive_feedback_by_type']), 2),
                num_neg = round(mean(value[key == 'page_negative_feedback']), 2)) %>%
      ungroup %>%
  mutate(ratio = num_pos / num_neg)

# remove Inf from data
x <- x[!grepl('Inf', x$ratio),]


p <- ggplot(x, aes(x = date, group = name, y = ratio, color = name)) + 
  geom_line(alpha = 0.6) +
  geom_smooth() +
  xlab('Page') +
  ylab('Ratio') +
  labs(title = 'Ratio of positive to negative interactions') +
  theme_databrew() + 
  theme(legend.position = 'none') + 
  theme(plot.title = element_text(size = 12), 
        legend.title=element_text(size=6), 
        legend.text=element_text(size=6)) +
  theme(axis.text=element_text(size=6), 
        axis.title=element_text(size=6), 
        axis.text.y = element_text(size =8),
        axis.text.x = element_text(angle = 45, hjust=1, size = 8),
        strip.text = element_text(size=8)) +
  facet_wrap( ~ name, ncol = 4, nrow = 4) +
  theme(strip.background = element_blank(),
   strip.text.y = element_blank())

p

```


Column {.tabset}
-------------------------------------
```{r}

x <- df[grepl('positive_feedback', df$key),]
x <- as.data.frame(x[grepl('like|comment', x$sub_key),])


# remove key
x$key <- x$end_time <- x$lat <- x$lon <-  NULL

# get first date where value_cum > 0
starter <- min(x$date[x$value_cum > 0], na.rm = TRUE)

x <- x %>%
  filter(date >= starter)
```

### All data

```{r}

page = 'all'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
  


```

### Art

```{r}

page = 'Art'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)

```

### Beauty

```{r}
page = 'Beauty'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Cheese

```{r}
page = 'Cheese'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Design

```{r}
page = 'Design'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Dessert

```{r}
page = 'Dessert'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Fitness

```{r}
page = 'Fitness'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Food

```{r}
page = 'Food'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Home

```{r}
page = 'Home'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Inventions

```{r}
page = 'Inventions'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Kitchen

```{r}
page = 'Kitchen'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Popculture

```{r}
page = 'Popculture'
div =1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)
```

### Science 

```{r}
page = 'Science'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)

```

### style 

```{r}
page = 'Style'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)

```

### ThisIsInsider

```{r}
page = 'Thisisinsider'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)

```

### Travel

```{r}
page = 'Travel'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)

```

### Video

```{r}
page = 'Video'
div = 1000000
p <- simple_chart(page = page, div = div)
ggplotly(p, tooltip = c('value_cum/div', 'sub_key')) %>% 
   layout(autosize = F, width = 550, height = 500)

```


Data {data-orientation=rows}
=====================================     
   
Row {data-height=300}
-------------------------------------

### Data repository

The dashboard automatically stores several types of data:
- A raw data snapshot for retrieval, archiving, and backups.
- A master dataset for analysis by data scientists.
- A daily google spreadsheet update for analysts: https://docs.google.com/spreadsheets/d/1Q7qZqSo_K-2d09xe4SDo5arZxIKd84-l4v4qm-BN0jM/edit#gid=0

Row {data-height=700}
-------------------------------------
   
### Data explorer

```{r}
# filtering down so that it's not too much data / too slow
DT::datatable(df[1:1000,] %>%
                dplyr::select(date, key, sub_key, name, value, value_cum))
```   
    
